{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Setup and input data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/orders/Order.all.20201001_20201031.csv\n/kaggle/input/orders/Order.all.20200901_20200930.csv\n/kaggle/input/orders/Order.all.20200801_20200831.csv\n/kaggle/input/orders/Order.all.20200701_20200731.csv\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install pyspark","execution_count":4,"outputs":[{"output_type":"stream","text":"Collecting pyspark\n  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n\u001b[K     |████████████████████████████████| 204.2 MB 26 kB/s s eta 0:00:01    |█████████▍                      | 59.8 MB 23.9 MB/s eta 0:00:07     |██████████████████████▊         | 145.3 MB 53.4 MB/s eta 0:00:02\n\u001b[?25hCollecting py4j==0.10.9\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n\u001b[K     |████████████████████████████████| 198 kB 38.5 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612244 sha256=8e63943bd8c5c5d4a96fa38998c2c9d3d4982963fa480675c443814dbf05b8b2\n  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.9 pyspark-3.0.1\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Import packages and setup Spark"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import monotonically_increasing_id\nfrom pyspark.sql.functions import col\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import ParamGridBuilder\nfrom pyspark.ml.tuning import CrossValidator\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nimport pandas as pd","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession \\\n    .builder \\\n    .appName(\"sales\") \\\n    .config(\"spark.driver.maxResultSize\", \"96g\") \\\n    .config(\"spark.driver.memory\", \"96g\") \\\n    .config(\"spark.executor.memory\", \"8g\") \\\n    .getOrCreate()","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data and data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(csv_file):\n  df = spark.read.csv(csv_file, header=True, inferSchema=True)\n  return df","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data from July, August and September\nsales_07 = read_data('../input/orders/Order.all.20200701_20200731.csv')\nsales_08 = read_data('../input/orders/Order.all.20200801_20200831.csv')\nsales_09 = read_data('../input/orders/Order.all.20200901_20200930.csv')\nsales_10 = read_data('../input/orders/Order.all.20201001_20201031.csv')","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only use the completed orders\ndef data_cleaning(df):\n  completed_orders = df.filter(col('Order Status') == 'Completed')\n  completed_orders = completed_orders.select('Username (Buyer)','Product Name','Quantity')\n  return completed_orders","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_07 = data_cleaning(sales_07)\ndf_08 = data_cleaning(sales_08)\ndf_09 = data_cleaning(sales_09)\ndf_10 = data_cleaning(sales_10)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_07.unionByName(df_08).unionByName(df_09).unionByName(df_10)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform user name to integer id\nusers = df.select('Username (Buyer)').distinct()\nusers = users.coalesce(1)\nusers = users.withColumn(\n\"userIntId\", monotonically_increasing_id()).persist()\n# users.show()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform product name to integer id\nitems = df.select('Product Name').distinct()\nitems = items.coalesce(1)\nitems = items.withColumn(\n\"itemIntId\", monotonically_increasing_id()).persist()\n# items.show()","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_w_int_ids = df.join(\nusers, \"Username (Buyer)\", \"left\").join(items, \"Product Name\", \"left\")\n# sales_w_int_ids.show()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_data = sales_w_int_ids.select(\n                                        col(\"userIntId\").alias(\"userId\"),\n                                        col(\"itemIntId\").alias(\"itemId\"),\n                                        col(\"Quantity\"))\n# sales_data.show()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users = sales_data.select(\"userId\").distinct()\nitems = sales_data.select(\"itemId\").distinct()\n\ncross_join = users.crossJoin(items).join(sales_data, [\"userId\", \"itemId\"], \"left\").fillna(0).persist()\n# cross_join.show()","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data sparsity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sparsity\nsparsity = 1 - sales_data.count()/(users.count()*items.count())","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Sparsity: \", sparsity)","execution_count":20,"outputs":[{"output_type":"stream","text":"Sparsity:  0.97233688950839\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Build models"},{"metadata":{"trusted":true},"cell_type":"code","source":"userCol = \"userId\"\nitemCol = \"itemId\"\nratingCol = \"Quantity\"","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/jamenlong/ALS_expected_percent_rank_cv/blob/master/ROEM_function.py\n\n#Expected percentile rank error metric function\ndef ROEM(predictions, userCol = userCol, itemCol = itemCol, ratingCol = ratingCol):\n  #Creates table that can be queried\n  predictions.createOrReplaceTempView(\"predictions\")\n\n  #Sum of total number of purchases of all products\n  denominator = predictions.groupBy().sum(ratingCol).collect()[0][0]\n\n  #Calculating rankings of products predictions by user\n  spark.sql(\"SELECT \" + userCol + \" , \" + ratingCol + \" , PERCENT_RANK() OVER (PARTITION BY \" + userCol + \" ORDER BY prediction DESC) AS rank FROM predictions\").createOrReplaceTempView(\"rankings\")\n\n  #Multiplies the rank of each product by the number of purchases and adds the products together\n  numerator = spark.sql('SELECT SUM(' + ratingCol + ' * rank) FROM rankings').collect()[0][0]\n\n  performance = numerator/denominator\n\n  return performance","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train, test) = cross_join.randomSplit([.8, .2], seed=12)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Empty list to be filled with models\nmodel_list = []\nparams_list = []\nroems = []\n\n\nranks = [3,4,5]\nmaxIters = [11,12,13]\nregParams = [0.005,0.01,0.015]\nalphas = [9,10,11]","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For loop will automatically create and store ALS models\nfor r in ranks:\n    for mi in maxIters:\n        for rp in regParams:\n            for a in alphas:\n                params_list.append({'rank': r, 'maxIter': mi, 'regParam': rp, 'alpha': a})\n                model_list.append(ALS(userCol= userCol, itemCol= itemCol, ratingCol= ratingCol, \n                                      rank = r, maxIter = mi, regParam = rp, alpha = a, \n                                      coldStartStrategy=\"drop\",nonnegative = True, implicitPrefs = True))","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(params_list)","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"81"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in model_list:\n  # Fits each model to the training data\n  trained_model = model.fit(train)\n  # Generates test predictions\n  predictions = trained_model.transform(test)\n  # Evaluates each model's performance\n  roems.append(ROEM(predictions))","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_params = pd.DataFrame(params_list)\ndf_params['ROEM'] = roems","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_params","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"    rank  maxIter  regParam  alpha      ROEM\n0      3       11     0.005      9  0.017085\n1      3       11     0.005     10  0.017085\n2      3       11     0.005     11  0.017085\n3      3       11     0.010      9  0.018059\n4      3       11     0.010     10  0.017085\n..   ...      ...       ...    ...       ...\n76     5       13     0.010     10  0.028449\n77     5       13     0.010     11  0.028449\n78     5       13     0.015      9  0.023378\n79     5       13     0.015     10  0.024324\n80     5       13     0.015     11  0.028449\n\n[81 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rank</th>\n      <th>maxIter</th>\n      <th>regParam</th>\n      <th>alpha</th>\n      <th>ROEM</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>11</td>\n      <td>0.005</td>\n      <td>9</td>\n      <td>0.017085</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>11</td>\n      <td>0.005</td>\n      <td>10</td>\n      <td>0.017085</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>11</td>\n      <td>0.005</td>\n      <td>11</td>\n      <td>0.017085</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>11</td>\n      <td>0.010</td>\n      <td>9</td>\n      <td>0.018059</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>11</td>\n      <td>0.010</td>\n      <td>10</td>\n      <td>0.017085</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>5</td>\n      <td>13</td>\n      <td>0.010</td>\n      <td>10</td>\n      <td>0.028449</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>5</td>\n      <td>13</td>\n      <td>0.010</td>\n      <td>11</td>\n      <td>0.028449</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>5</td>\n      <td>13</td>\n      <td>0.015</td>\n      <td>9</td>\n      <td>0.023378</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>5</td>\n      <td>13</td>\n      <td>0.015</td>\n      <td>10</td>\n      <td>0.024324</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>5</td>\n      <td>13</td>\n      <td>0.015</td>\n      <td>11</td>\n      <td>0.028449</td>\n    </tr>\n  </tbody>\n</table>\n<p>81 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_params.iloc[np.argmin(df_params.ROEM)]","execution_count":53,"outputs":[{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"rank         4.000000\nmaxIter     12.000000\nregParam     0.010000\nalpha       10.000000\nROEM         0.009318\nName: 40, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_rank = df_params.iloc[np.argmin(df_params.ROEM)][0]\nbest_maxiter = df_params.iloc[np.argmin(df_params.ROEM)][1]\nbest_regparam = df_params.iloc[np.argmin(df_params.ROEM)][2]\nbest_alpha = df_params.iloc[np.argmin(df_params.ROEM)][3]","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = ALS(userCol= userCol, itemCol= itemCol, ratingCol= ratingCol, \n                                      rank = best_rank, maxIter = best_maxiter, regParam = best_regparam, alpha = best_alpha, \n                                      coldStartStrategy=\"drop\",nonnegative = True, implicitPrefs = True)","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_best_model = best_model.fit(cross_join)\npredictions = fit_best_model.transform(cross_join)","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.take(10)","execution_count":58,"outputs":[{"output_type":"stream","text":"+------+------+--------+------------+\n|userId|itemId|Quantity|  prediction|\n+------+------+--------+------------+\n|   148|    31|       0|  0.06389766|\n|   463|    31|       0|6.0582574E-4|\n|   471|    31|       0|6.0582574E-4|\n|   496|    31|       0|         0.0|\n|   243|    31|       0|6.0582574E-4|\n|   392|    31|       0|  0.02451766|\n|   540|    31|       0|6.0582574E-4|\n|   623|    31|       0|         0.0|\n|   737|    31|       0|   0.0659067|\n|    31|    31|       0|6.0582574E-4|\n|   516|    31|       0|6.0582574E-4|\n|    85|    31|       0|         0.0|\n|   137|    31|       0|  0.06555372|\n|   251|    31|       0|         0.0|\n|   451|    31|       0|         0.0|\n|   580|    31|       0|6.0582574E-4|\n|   808|    31|       0| 6.363547E-4|\n|    65|    31|       0|6.0582574E-4|\n|   458|    31|       0| 6.363547E-4|\n|    53|    31|       0|6.0582574E-4|\n+------+------+--------+------------+\nonly showing top 20 rows\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}